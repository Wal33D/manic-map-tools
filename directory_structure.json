{".env.local":{"content":"MMT_CATALOG_DIR=C://Users//Aquataze//Desktop//discordChannelBot//catalog MMT_ARCHIVED_CATALOG_DIR=C://Users//Aquataze//Desktop//discordChannelBot//catalog//archived HOGNOSE_LEVELS_CATALOG_DIR=C://Users//Aquataze//Desktop//discordChannelBot//catalog//hognose HOGNOSE_LEVELS_DOWNLOAD_DIR=C://Users//Aquataze//Desktop//discordChannelBot//hognoseDownloads MMT_CLEAN_ME_DIR=C://Users//Aquataze//Desktop//discordChannelBot//processedCatalog MMT_GENERATE_PNG_DIR=C://Users//Aquataze//Desktop//discordChannelBot//hognoseCatalog DB_USERNAME=manic-miner-map-uploader DB_PASSWORD=UR8A2UhjDdJFlGWHebQN00XKpjbn3wak DB_CLUSTER=@catalog.qehknvs.mongodb.net HOGNOSE_LEVELS_TEST_DL_DIR=C://Users//Aquataze//Desktop//discordChannelBot//testdl"},".gitignore":{"content":"dist .vscode node_modules samples .env .env.local"},"fileParser":{"mapFileParser.ts":{"content":"import fs from \"fs\"; import iconv from \"iconv-lite\"; import chardet from \"chardet\"; import { ParsedMapData } from \"./types\"; import { countResources, getSizeCategory } from \"./utils\"; /** * Parses map data from a file to extract detailed game map information. * This module reads a map file and parses its contents into a structured format. * Each map file should contain details about rows, columns, and resource distributions * across different sections like tiles, height, ore, and crystals. * * @module parseMapDataFromFile * * Parameters in ParsedMapData: * - rowcount: number of rows in the map * - colcount: number of columns in the map * - size: total number of tiles (rowcount * colcount) * - longestDimension: longer dimension between rowcount and colcount * - shortestDimension: shorter dimension between rowcount and colcount * - orientation: 'x' for horizontal, 'y' for vertical orientation * - maxElevation: highest elevation in the map * - minElevation: lowest elevation in the map * - averageElevation: average elevation across the map * - elevationRange: difference between max and min elevation * - oreCount: total quantity of ore across the map * - crystalCount: total quantity of crystals across the map * - biome: type of biome the map represents * - creator: creator of the map * - levelname: name of the level * - tilesArray: array of tile IDs * - heightArray: array of elevation values * - oreArray: array of ore quantities per tile * - crystalArray: array of crystal quantities per tile * - creatures: list of creatures in the map * - miners: list of miners in the map * - briefing: mission briefing text * - briefingsuccess: success briefing text * - briefingfailure: failure briefing text * - vehicles: list of vehicles in the map */ export function parseMapDataFromFile({ filePath, }: { filePath: string; }): Promise<ParsedMapData> { return new Promise((resolve, reject) => { // Detect the file encoding const encoding = chardet.detectFileSync(filePath) || \"utf8\"; fs.readFile(filePath, (err, data) => { if (err) { console.error(\"Failed to read file\", err); reject(err); return; } // Decode the buffer using the detected encoding const levelFileData = iconv.decode(data, encoding); const parsedData: ParsedMapData = { size: 0, rowcount: 0, colcount: 0, longestDimension: 0, shortestDimension: 0, axisCount: 0, maxElevation: 0, minElevation: Number.MAX_SAFE_INTEGER, averageElevation: 0, elevationRange: 0, oreCount: 0, crystalCount: 0, isSquare: false, biome: \"\", creator: \"\", levelname: \"\", sizeCategory: \"\", tilesArray: [], heightArray: [], oreArray: [], crystalArray: [], creatures: \"\", miners: \"\", briefing: \"\", briefingsuccess: \"\", briefingfailure: \"\", vehicles: \"\", }; const lines = levelFileData .split(\"\\n\") .map((line) => line.trim()) .filter((line) => line.length > 0); let currentKey = \"\"; let resourceKey = \"\"; lines.forEach((line) => { if (line.endsWith(\"{\")) { currentKey = line.replace(\"{\", \"\").trim(); } else if (line.startsWith(\"}\")) { currentKey = \"\"; resourceKey = \"\"; } else if (currentKey === \"info\") { const keyValue = line.split(\":\"); const key = keyValue[0].trim().toLowerCase(); const value = keyValue[1].trim(); if (key === \"rowcount\" || key === \"colcount\") { parsedData[key] = parseInt(value, 10); } else if ( key === \"biome\" || key === \"creator\" || key === \"levelname\" ) { parsedData[key] = value; } } else if (currentKey === \"resources\") { if (line.includes(\":\")) { resourceKey = line.split(\":\")[0].trim(); } else if (resourceKey) { const numbers = line .split(\",\") .filter((n) => n.trim() !== \"\") .map((n) => parseInt(n, 10)) .filter((n) => !isNaN(n)); if (resourceKey === \"crystals\") { parsedData.crystalArray = parsedData.crystalArray.concat(numbers); } else if (resourceKey === \"ore\") { parsedData.oreArray = parsedData.oreArray.concat(numbers); } } } else if (currentKey === \"tiles\" || currentKey === \"height\") { const numbers = line .split(\",\") .filter((n) => n.trim() !== \"\") .map((n) => parseInt(n, 10)) .filter((n) => !isNaN(n)); if (currentKey === \"tiles\") { parsedData.tilesArray = parsedData.tilesArray.concat(numbers); } else if (currentKey === \"height\") { parsedData.heightArray = parsedData.heightArray.concat(numbers); } } else if ( currentKey === \"creatures\" || currentKey === \"miners\" || currentKey === \"briefing\" || currentKey === \"briefingsuccess\" || currentKey === \"briefingfailure\" || currentKey === \"vehicles\" ) { parsedData[currentKey] += line + \"\\n\"; } }); parsedData.oreCount = countResources(parsedData.oreArray); parsedData.crystalCount = countResources(parsedData.crystalArray); if (parsedData.heightArray.length > 0) { parsedData.maxElevation = Math.max(...parsedData.heightArray); parsedData.minElevation = Math.min(...parsedData.heightArray); parsedData.averageElevation = parsedData.heightArray.reduce((acc, val) => acc + val, 0) / parsedData.heightArray.length; parsedData.elevationRange = parsedData.maxElevation - parsedData.minElevation; } parsedData.size = parsedData.rowcount * parsedData.colcount; parsedData.sizeCategory = getSizeCategory(parsedData.size); parsedData.longestDimension = Math.max( parsedData.rowcount, parsedData.colcount ); parsedData.shortestDimension = Math.min( parsedData.rowcount, parsedData.colcount ); parsedData.axisCount = parsedData.rowcount < parsedData.colcount ? parsedData.rowcount : parsedData.colcount; parsedData.isSquare = parsedData.rowcount === parsedData.colcount; resolve(parsedData); }); }); }"},"types.ts":{"content":"export interface ParsedMapData { rowcount: number; colcount: number; size: number; longestDimension: number; shortestDimension: number; axisCount: number; maxElevation: number; minElevation: number; averageElevation: number; elevationRange: number; oreCount: number; crystalCount: number; isSquare: boolean; biome: string; creator: string; vehicles: string; miners: string; creatures: string; briefingsuccess: string; briefingfailure: string; briefing: string; levelname: string; sizeCategory: string; tilesArray: number[]; heightArray: number[]; oreArray: number[]; crystalArray: number[]; }"},"utils.ts":{"content":"export function countResources(resourceArray: any[]) { return resourceArray.flat().reduce((count: number, value: number) => { if (value > 0) { return count + value; } return count; }, 0); } export function getSizeCategory(size: number) { const averageSize = 2808.509; const smallThreshold = averageSize * 0.66; const largeThreshold = averageSize * 1.5; if (size < smallThreshold) return \"small\"; if (size > largeThreshold) return \"large\"; return \"medium\"; }"}},"index.ts":{"content":"import * as dotenv from \"dotenv\"; dotenv.config({ path: \".env.local\" }); async function init() { console.log(\"========== Manic Miners Tools Overview ==========\\n\"); console.log(\"Project Name: Manic Miners Tools\"); console.log(\"Version: 1.0.0\"); console.log(\"Author: Waleed Judah\"); console.log(\"License: MIT\"); console.log(\"\\n\"); } init().catch((err) => console.error(\"[ERROR] Error initializing project overview:\", err) );"},"package.json":{"content":"{ \"name\": \"manic-map-tools\", \"version\": \"1.0.0\", \"main\": \"index.js\", \"scripts\": { \"resetdist\": \"rimraf dist\", \"build\": \"tsc\", \"copy-assets\": \"copyfiles -u 1 ./assets/**/* ./dist\", \"prep\": \"npm run resetdist && npm run build && npm run copy-assets\", \"generate\": \"npm run prep && node dist/scripts/generatePNG.js\", \"check\": \"npm run prep && node dist/scripts/mapIntegrityCheck.js\", \"avg\": \"npm run prep && node dist/scripts/averageMapSize.js\", \"stats\": \"npm run prep && node dist/scripts/logMapDataStats.js\", \"clean\": \"npm run prep && node dist/scripts/cleanMapFile.js\", \"buildCatalog\": \"npm run prep && node dist/scripts/buildArchivedLevelCatalog.js\", \"downloadArchivedFiles\": \"npm run prep && node dist/scripts/downloadArchivedLevelData.js\", \"buildArchived\": \"npm run buildCatalog && node dist/scripts/downloadArchivedLevelData.js\", \"fetcHognose\": \"npm run prep && node dist/scripts/fetchHognoseMaps.js\", \"catalogHognose\": \"npm run prep && node dist/scripts/catalogHognoseMaps.js\", \"buildHognose\": \"npm run fetcHognose && node dist/scripts/catalogHognoseMaps.js\", \"minify\": \"npm run prep && node dist/scripts/minifyProject.js\", \"uploadHognoseCatalog\": \"npm run prep && node dist/scripts/db/uploadHognoseCatalog.js\", \"downloadCollectionFromMongoDB\": \"npm run prep && node dist/scripts/db/downloadCollectionFromMongoDB.js\", \"start\": \"ts-node index.ts\" }, \"author\": \"Waleed Judah\", \"license\": \"MIT\", \"description\": \"A comprehensive toolset for processing, analyzing, and visualizing map data for Manic Miners.\", \"dependencies\": { \"axios\": \"^1.6.8\", \"canvas\": \"^2.11.2\", \"chardet\": \"^2.0.0\", \"dotenv\": \"^16.4.5\", \"mongodb\": \"^6.6.2\", \"node-fetch\": \"^3.3.2\", \"node-html-parser\": \"^6.1.13\", \"unzipper\": \"^0.11.6\", \"xml2js\": \"^0.6.2\" }, \"devDependencies\": { \"@types/chardet\": \"^0.8.3\", \"@types/dotenv\": \"^8.2.0\", \"@types/node\": \"^20.12.12\", \"@types/unzipper\": \"^0.10.9\", \"@types/xml2js\": \"^0.4.14\", \"copyfiles\": \"^2.4.1\" } }"},"scripts":{"averageMapSize.ts":{"content":"import * as fs from \"fs/promises\"; import * as path from \"path\"; import * as dotenv from \"dotenv\"; import * as readline from \"readline\"; dotenv.config({ path: \".env.local\" }); const rl = readline.createInterface({ input: process.stdin, output: process.stdout, }); async function calculateMapSizeStats(baseDir: string): Promise<any> { let totalSize = 0; let fileCount = 0; let failedCount = 0; let minSize = Infinity; let maxSize = -Infinity; let failedFiles: string[] = []; let emptyDatDirectories: string[] = []; let directoriesChecked = 0; let directoriesWithDatFiles = 0; async function traverseDirectory(directory: string): Promise<boolean> { directoriesChecked++; let datFileFound = false; try { const directoryContents = await fs.readdir(directory, { withFileTypes: true, }); for (const dirent of directoryContents) { const fullPath = path.join(directory, dirent.name); if (dirent.isDirectory()) { const isEmpty = await traverseDirectory(fullPath); if (isEmpty) { emptyDatDirectories.push(fullPath); } } else if (dirent.name.endsWith(\".dat\")) { datFileFound = true; try { const data = await fs.readFile(fullPath, \"utf8\"); const size = parseMapSize(data); if (size !== null) { totalSize += size; fileCount++; if (size < minSize) minSize = size; if (size > maxSize) maxSize = size; } else { failedCount++; failedFiles.push(fullPath); } } catch (readError) { console.error( `[ERROR] Error reading file ${fullPath}: ${readError}` ); failedCount++; failedFiles.push(fullPath); } } } } catch (dirError) { console.error( `[ERROR] Error accessing directory ${directory}: ${dirError}` ); } if (datFileFound) { directoriesWithDatFiles++; } return !datFileFound; } function parseMapSize(fileContent: string): number | null { const rowMatch = fileContent.match(/rowcount:\\s*(\\d+)/); const colMatch = fileContent.match(/colcount:\\s*(\\d+)/); if (rowMatch && colMatch) { return parseInt(rowMatch[1], 10) * parseInt(colMatch[1], 10); } return null; } const isEmpty = await traverseDirectory(baseDir); if (isEmpty) { emptyDatDirectories.push(baseDir); } const averageSize = fileCount > 0 ? (totalSize / fileCount).toFixed(2) : 0; const result = { processedFiles: fileCount, failedFiles: failedCount, directoriesChecked, directoriesWithDatFiles, averageSize, minSize, maxSize, failedFilesDetails: failedFiles, emptyDatDirectories, }; console.log(\"========== Manic Miners Map Tool Statistics ==========\"); console.log(`Processed files: ${fileCount}`); console.log(`Failed to process files: ${failedCount}`); console.log(`Directories checked: ${directoriesChecked}`); console.log(`Directories with .dat files: ${directoriesWithDatFiles}`); if (failedCount > 0) { console.log(\"Failed file paths:\"); failedFiles.forEach((file) => console.log(file)); } if (emptyDatDirectories.length > 0) { console.log(\"Directories without .dat files:\"); emptyDatDirectories.forEach((dir) => console.log(dir)); } console.log(`Average map size: ${averageSize}`); console.log(`Minimum map size: ${minSize}`); console.log(`Maximum map size: ${maxSize}`); console.log(\"======================================================\"); return result; } async function init() { try { const directoryPath = process.env.MMT_CATALOG_DIR; rl.question( `The directory to be processed is: ${directoryPath}. Would you like to proceed? (yes/no): \\n`, async (answer) => { if (answer.toLowerCase() === \"yes\") { const processingResults = await calculateMapSizeStats(directoryPath); console.log(processingResults); } else { console.log(\"[INFO] Process aborted by user.\"); } rl.close(); } ); } catch (err) { console.error( \"[ERROR] Error initializing map size stats calculation:\", err ); } } init();"},"buildArchivedLevelCatalog.ts":{"content":"import fs from \"fs\"; import path from \"path\"; import axios from \"axios\"; import * as dotenv from \"dotenv\"; import { parseXmlToJson } from \"../utils/parseXmlToJson\"; import { camelCaseString } from \"../utils/camelCaseString\"; import { parseCatalogXmlToJson } from \"../utils/parseCatalogXmlToJson\"; dotenv.config({ path: \".env.local\" }); const baseUrl = \"https://archive.org/advancedsearch.php\"; const CACHE_FILENAME = \"catalog_index.json\"; const REQUEST_DELAY = 1500; // Delay between requests in milliseconds const MAX_RETRIES = 3; // Maximum number of retries for a request const CATALOG_DIR: string = process.env.MMT_ARCHIVED_CATALOG_DIR; const ensureDirectoryExists = async (dir: fs.PathLike) => { try { await fs.promises.stat(dir); } catch { await fs.promises.mkdir(dir, { recursive: true }); } }; const readCacheFile = async (filePath: string): Promise<any> => { try { const cacheFile = await fs.promises.readFile(filePath, \"utf8\"); return JSON.parse(cacheFile); } catch { return { catalog: \"Archived\", catalogType: \"Level\", entries: [] }; } }; const writeCacheFile = async (filePath: string, data: any) => { await fs.promises.writeFile(filePath, JSON.stringify(data, null, 2), \"utf8\"); }; const saveMetadata = async ( name: string, metadataXml: string, files: any[], thumbnailUrl: string ) => { const camelCasedName = camelCaseString(name); const dirPath = path.join(CATALOG_DIR, camelCasedName); await ensureDirectoryExists(dirPath); const filePath = path.join(dirPath, \"catalog.json\"); const metadataJson = await parseCatalogXmlToJson( metadataXml, dirPath, files, thumbnailUrl ); await fs.promises.writeFile( filePath, JSON.stringify(metadataJson, null, 2), \"utf8\" ); }; const fetchWithRetries = async (url: string, retries: number = MAX_RETRIES) => { for (let attempt = 1; attempt <= retries; attempt++) { try { const response = await axios.get(url); return response; } catch (error) { if (attempt === retries) { throw error; } console.warn(`Request failed (attempt ${attempt}): ${error.message}`); await new Promise((resolve) => setTimeout(resolve, REQUEST_DELAY)); } } }; const fetchLevelsData = async ( queryString = \"manic miners\" ): Promise<any[]> => { await ensureDirectoryExists(CATALOG_DIR); const cacheFilePath = path.join(CATALOG_DIR, CACHE_FILENAME); let cachedData = await readCacheFile(cacheFilePath); const cachedIdentifiers = new Set( cachedData.entries.map((entry: any) => entry.catalogId) ); const query = encodeURIComponent(queryString); const fields = [ \"title\", \"identifier\", \"creator\", \"date\", \"description\", \"downloads\", ].join(\"&fl[]=\"); const fullUrl = `${baseUrl}?q=${query}&fl[]=${fields}&mediatype='software'&rows=99999999&page=1&output=json&callback=callback`; try { const { data: text } = await fetchWithRetries(fullUrl); const jsonp = text.substring(text.indexOf(\"(\") + 1, text.lastIndexOf(\")\")); const data = JSON.parse(jsonp); const filteredResults = data.response.docs.filter( (doc: { title: string; identifier: string }) => doc.title.toLowerCase().includes(\"manic\") || doc.identifier.toLowerCase().includes(\"manic\") ); for (const doc of filteredResults.filter((doc: { identifier: string }) => doc.identifier.startsWith(\"ManicMiners-level-\") )) { if (cachedIdentifiers.has(doc.identifier)) continue; const levelData = { catalog: \"Archived\", catalogType: \"Level\", archived: false, pre_release: false, catalogId: doc.identifier, title: doc.title.split(\"|\")[0].trim() || \"No title\", postedDate: doc.date || \"No date\", author: doc.creator || \"No author\", htmlDescription: doc.description || \"No description\", thumbnail: \"\", screenshot: \"\", metadataUrl: `https://archive.org/download/${doc.identifier}/${doc.identifier}_meta.xml`, downloadUrl: \"\", fileListUrl: `https://archive.org/download/${doc.identifier}/${doc.identifier}_files.xml`, }; try { const { data: xmlText } = await fetchWithRetries(levelData.fileListUrl); const xmlData = parseXmlToJson(xmlText); const datFile: any = xmlData.find((file: any) => file.format === \"DAT\"); if (datFile) levelData.downloadUrl = `https://archive.org/download/${doc.identifier}/${datFile.name}`; const thumbnails: any = []; const files = xmlData .filter((file: any) => { if (file.format === \"Item Tile\" || file.format === \"JPEG Thumb\") { thumbnails.push(file); return false; } return ( !file.name.endsWith(\".torrent\") && !file.name.endsWith(\".sqlite\") && !file.name.endsWith(\"_meta.xml\") && !file.name.endsWith(\"_files.xml\") ); }) .map((file: any) => ({ fileName: file.name, fileType: file.format, fileSize: file.size, fileUrl: `https://archive.org/download/${doc.identifier}/${file.name}`, })); if (thumbnails.length) { const largestThumbnail = thumbnails.reduce( (prev: { size: string }, current: { size: string }) => parseInt(current.size, 10) > parseInt(prev.size, 10) ? current : prev ); levelData.thumbnail = `https://archive.org/download/${doc.identifier}/${largestThumbnail.name}`; } // Fetch metadata XML and save it as JSON with files and thumbnailUrl const { data: metadataXml } = await fetchWithRetries( levelData.metadataUrl ); await saveMetadata( levelData.title, metadataXml, files, levelData.thumbnail ); cachedData.entries.push({ catalogId: levelData.catalogId, directory: camelCaseString(levelData.title), hasScreenshot: !!levelData.thumbnail, hasThumbnail: !!levelData.thumbnail, hasDatFile: !!levelData.downloadUrl, hasCatalogJson: true, }); await writeCacheFile(cacheFilePath, cachedData); await new Promise((resolve) => setTimeout(resolve, REQUEST_DELAY)); } catch (error) { console.error(`Error processing ${doc.identifier}:`, error); } } } catch (error) { console.error(\"Error fetching data from the internet archive:\", error); } return cachedData; }; const init = async () => { const processingResults = await fetchLevelsData(\"manic miners\"); console.log(\"Processing results:\"); console.log(processingResults); }; init(); export { fetchLevelsData };"},"catalogHognoseMaps.ts":{"content":"import fs from \"fs\"; import path from \"path\"; import * as dotenv from \"dotenv\"; import { generatePNG } from \"../src/functions/generatePNG\"; import { create2DArray } from \"../src/functions/create2DArray\"; import { generateThumbnail } from \"../src/functions/generateThumbnail\"; import { parseMapDataFromFile } from \"../fileParser/mapFileParser\"; import { generateShortDescription } from \"../utils/generateShortDescription\"; import { constructDescription, constructHtmlDescription, } from \"../utils/constructDescriptions\"; dotenv.config({ path: \".env.local\" }); const { HOGNOSE_LEVELS_DOWNLOAD_DIR: DOWNLOAD_DIR, HOGNOSE_LEVELS_CATALOG_DIR: CATALOG_DIR, } = process.env; if (!DOWNLOAD_DIR || !CATALOG_DIR) { console.error( \"Error: Both HOGNOSE_LEVELS_DOWNLOAD_DIR and HOGNOSE_LEVELS_CATALOG_DIR must be set in the .env file.\" ); process.exit(1); } const INDEX_FILE_PATH = path.resolve(DOWNLOAD_DIR, \"hognose_index.json\"); const CATALOG_INDEX_FILE_PATH = path.resolve(CATALOG_DIR, \"catalog_index.json\"); const extractSeed = (filePath: string): string => fs.readFileSync(filePath, \"utf8\").match(/seed:\\s*(0x[0-9a-fA-F]+)/)?.[1] || \"Unknown\"; const createCatalogData = ( parsedData: any, datFile: string, targetDir: string, indexData: any ) => ({ catalog: \"Hognose\", catalogType: \"Level\", archived: false, pre_release: false, seed: extractSeed(path.resolve(targetDir, datFile)), catalogId: path.basename(targetDir), title: parsedData.levelname || datFile.replace(\".dat\", \"\"), postedDate: indexData.downloadedAt, author: \"charredUtensil\", mainFile: datFile, thumbnail: \"thumbnail_render.png\", screenshot: \"screenshot_render.png\", shortDescription: generateShortDescription(parsedData), textDescription: constructDescription(parsedData), htmlDescription: constructHtmlDescription(parsedData), path: targetDir, url: \"https://github.com/charredUtensil/hognose\", files: [ { fileName: datFile, fileSize: `${( fs.statSync(path.resolve(targetDir, datFile)).size / 1024 ).toFixed(2)} KB`, }, ], }); async function organizeDatFiles() { try { if (!fs.existsSync(INDEX_FILE_PATH)) throw new Error(`Index file not found at ${INDEX_FILE_PATH}`); const indexData = JSON.parse(fs.readFileSync(INDEX_FILE_PATH, \"utf8\")); let catalogIndex = fs.existsSync(CATALOG_INDEX_FILE_PATH) ? JSON.parse(fs.readFileSync(CATALOG_INDEX_FILE_PATH, \"utf8\")) : { catalog: \"Hognose\", catalogType: \"Level\", entries: [] }; const processedFiles = new Set( catalogIndex.entries.map((entry: any) => entry.catalogId) ); for (const datFile of indexData.datFiles) { const targetDirName = datFile.replace(\".dat\", \"\"); if (processedFiles.has(targetDirName)) { console.log(`Skipping already processed file: ${datFile}`); continue; } const sourcePath = path.resolve(DOWNLOAD_DIR, datFile); const targetDir = path.resolve(CATALOG_DIR, targetDirName); const targetPath = path.resolve(targetDir, datFile); if (!fs.existsSync(targetDir)) fs.mkdirSync(targetDir, { recursive: true }); fs.copyFileSync(sourcePath, targetPath); console.log(`Copied ${datFile} to ${targetDir}`); const parsedData = await parseMapDataFromFile({ filePath: sourcePath }); const wallArray = create2DArray({ data: parsedData.tilesArray, width: parsedData.colcount, }); const thumbnailBuffer = await generateThumbnail(wallArray); const thumbnailPath = path.resolve(targetDir, \"thumbnail_render.png\"); fs.writeFileSync(thumbnailPath, thumbnailBuffer); const pngBuffer = await generatePNG(wallArray, parsedData.biome); const pngPath = path.resolve(targetDir, \"screenshot_render.png\"); await pngBuffer.toFile(pngPath); const catalogData = createCatalogData( parsedData, datFile, targetDir, indexData ); fs.writeFileSync( path.resolve(targetDir, \"catalog.json\"), JSON.stringify(catalogData, null, 2) ); console.log(`Created catalog.json in ${targetDir}`); catalogIndex.entries.push({ catalogId: catalogData.catalogId, directory: `${path.relative(CATALOG_DIR, targetDir)}`, hasScreenshot: fs.existsSync(pngPath), hasThumbnail: fs.existsSync(thumbnailPath), hasDatFile: fs.existsSync(targetPath), hasCatalogJson: fs.existsSync(path.resolve(targetDir, \"catalog.json\")), }); fs.writeFileSync( CATALOG_INDEX_FILE_PATH, JSON.stringify(catalogIndex, null, 2) ); console.log(`Updated catalog_index.json in ${CATALOG_DIR}`); } console.log( \"All .dat files have been organized into the catalog directory.\" ); } catch (error) { console.error(\"Error organizing .dat files:\", error.message); throw error; } } (async () => { try { if (!fs.existsSync(CATALOG_DIR)) fs.mkdirSync(CATALOG_DIR, { recursive: true }); await organizeDatFiles(); } catch (error) { console.error(\"Failed to organize .dat files:\", error.message); } })();"},"cleanMapFile.ts":{"content":"import * as fs from \"fs\"; import * as path from \"path\"; import * as chardet from \"chardet\"; import * as iconv from \"iconv-lite\"; import * as dotenv from \"dotenv\"; import * as readline from \"readline\"; dotenv.config({ path: \".env.local\" }); const rl = readline.createInterface({ input: process.stdin, output: process.stdout, }); function cleanKey(key: string, filePath: string): string { if (key.toLowerCase() === \"rowcount\" || key.toLowerCase() === \"colcount\") { console.error( `[WARNING] Found key with uppercase letters: ${key} --> in file: ${filePath}` ); return key.toLowerCase(); } return key; } const cleanMapFile = (filePath: string): void => { const encoding = chardet.detectFileSync(filePath) || \"utf8\"; console.log(`Detected file encoding: ${encoding}`); fs.readFile(filePath, (err, data) => { if (err) { console.error(`[ERROR] Error reading file ${filePath}:`, err); return; } const fileContent = iconv.decode(data, encoding as BufferEncoding); const cleanFileContent = (content: string): string => { const printableContent = content.replace(/[^\\x20-\\x7E\\n\\r]/g, \"\"); const cleanedContent = printableContent .replace(/\\r\\n/g, \"\\n\") .replace(/\\r/g, \"\\n\"); return cleanedContent .split(\"\\n\") .map((line) => { const keyValue = line.split(\":\"); if (keyValue.length === 2) { const key = keyValue[0].trim(); const value = keyValue[1].trim(); return `${cleanKey(key, filePath)}: ${value}`; } return line; }) .join(\"\\n\"); }; const cleanedData = cleanFileContent(fileContent); const backupFilePath = `${filePath}.bak`; fs.rename(filePath, backupFilePath, (renameErr) => { if (renameErr) { console.error( `[ERROR] Error renaming file ${filePath} to ${backupFilePath}:`, renameErr ); return; } fs.writeFile(filePath, iconv.encode(cleanedData, \"utf8\"), (writeErr) => { if (writeErr) { console.error( `[ERROR] Error writing cleaned content to ${filePath}:`, writeErr ); return; } console.log( `[INFO] Cleaned file content saved to ${filePath} for inspection.` ); const extractTilesArray = (content: string): number[] | null => { try { const tilesMatch = content.match(/tiles\\s*\\{\\s*([^}]*)\\s*\\}/); if (tilesMatch) { const tilesString = tilesMatch[1]; const tilesArray = tilesString .split(/[\\s,]+/) .map((num) => num.trim()) .filter((num) => num.length > 0 && num !== \"-\") .map((num) => parseInt(num, 10)) .filter((num) => !isNaN(num)); return tilesArray; } return null; } catch (error) { console.error(`[ERROR] Error extracting tiles array:`, error); return null; } }; const tilesArray = extractTilesArray(cleanedData); if (tilesArray) { console.log( `[INFO] Successfully extracted and parsed tiles from ${filePath}` ); } else { console.log(`[FAIL] Failed to extract tiles from file: ${filePath}`); } }); }); }); }; const getDatFiles = async (dirPath: string): Promise<string[]> => { const datFiles: string[] = []; const traverseDirectories = async (currentPath: string) => { const files = await fs.promises.readdir(currentPath, { withFileTypes: true, }); for (const file of files) { const filePath = path.join(currentPath, file.name); if (file.isDirectory()) { await traverseDirectories(filePath); } else if (file.isFile() && path.extname(filePath) === \".dat\") { datFiles.push(filePath); } } }; await traverseDirectories(dirPath); return datFiles; }; const processDatFiles = (filePaths: string[]): void => { filePaths.forEach(cleanMapFile); }; async function init() { try { const directoryPath = process.env.MMT_CLEAN_ME_DIR; const datFiles = await getDatFiles(directoryPath); if (datFiles.length === 0) { console.log(\"[INFO] No .dat files found to process.\"); rl.close(); return; } console.log(`\\nFiles to be processed:\\n${datFiles.join(\"\\n\")}\\n`); rl.question( `There are ${datFiles.length} files to process. Would you like to proceed? (yes/no): \\n`, (answer) => { if (answer.toLowerCase() === \"yes\") { processDatFiles(datFiles); } else { console.log(\"[INFO] Process aborted by user.\"); } rl.close(); } ); } catch (err) { console.error(\"[ERROR] Error initializing clean map files:\", err); } } init();"},"db":{"downloadCollectionFromMongoDB.ts":{"content":"import fs from \"fs\"; import path from \"path\"; import * as dotenv from \"dotenv\"; import { MongoClient } from \"mongodb\"; dotenv.config({ path: \".env.local\" }); const DB_USERNAME = process.env.DB_USERNAME; const DB_PASSWORD = process.env.DB_PASSWORD; const DB_CLUSTER = process.env.DB_CLUSTER; const DATABASE_NAME = process.env.DATABASE_NAME || \"levelsCatalogDB\"; const DOWNLOAD_DIR: any = process.env.HOGNOSE_LEVELS_TEST_DL_DIR; if (!DB_USERNAME || !DB_PASSWORD || !DB_CLUSTER || !DOWNLOAD_DIR) { console.error( \"Error: DB_USERNAME, DB_PASSWORD, DB_CLUSTER, HOGNOSE_LEVELS_TEST_DL_DIR, and COLLECTION_NAME must be set in the .env.local file.\" ); process.exit(1); } const MONGODB_URI = `mongodb+srv://${DB_USERNAME}:${DB_PASSWORD}${DB_CLUSTER}/${DATABASE_NAME}?retryWrites=true&w=majority`; async function downloadCollectionFromMongoDB(collectionName = \"Hognose\") { const client = new MongoClient(MONGODB_URI, {}); try { console.log(\"Connecting to MongoDB...\"); await client.connect(); const db = client.db(DATABASE_NAME); const collection = db.collection(collectionName); console.log(`Downloading documents from collection: ${collectionName}`); const documents = await collection.find().toArray(); for (const doc of documents) { const levelDir = path.join(DOWNLOAD_DIR, doc.levelName); if (!fs.existsSync(levelDir)) { fs.mkdirSync(levelDir, { recursive: true }); } for (const file of doc.files) { if (file.type === \"file\" && file.content) { const filePath = path.join(levelDir, file.name); fs.writeFileSync(filePath, Buffer.from(file.content, \"base64\")); console.log(`File written: ${filePath}`); } } } console.log(\"Download completed successfully.\"); } catch (error) { console.error(\"Error downloading from MongoDB:\", error.message); } finally { await client.close(); } } downloadCollectionFromMongoDB(\"Hognose\");"},"uploadHognoseCatalog.ts":{"content":"import fs from \"fs\"; import path from \"path\"; import * as dotenv from \"dotenv\"; import { MongoClient } from \"mongodb\"; dotenv.config({ path: \".env.local\" }); const DB_USERNAME = process.env.DB_USERNAME; const DB_PASSWORD = process.env.DB_PASSWORD; const DB_CLUSTER = process.env.DB_CLUSTER; const DATABASE_NAME = process.env.DATABASE_NAME || \"levelsCatalogDB\"; const ROOT_DIRECTORY_PATH = process.env.HOGNOSE_LEVELS_CATALOG_DIR; if (!DB_USERNAME || !DB_PASSWORD || !DB_CLUSTER || !ROOT_DIRECTORY_PATH) { console.error( \"Error: DB_USERNAME, DB_PASSWORD, DB_CLUSTER, and HOGNOSE_LEVELS_CATALOG_DIR must be set in the .env.local file.\" ); process.exit(1); } const MONGODB_URI = `mongodb+srv://${DB_USERNAME}:${DB_PASSWORD}${DB_CLUSTER}/${DATABASE_NAME}?retryWrites=true&w=majority`; async function uploadAllCatalogsToMongoDB() { const client = new MongoClient(MONGODB_URI, {}); try { console.log(\"Connecting to MongoDB...\"); await client.connect(); const db = client.db(DATABASE_NAME); console.log(`Traversing directory: ${ROOT_DIRECTORY_PATH}`); const catalogDirs = findCatalogDirectories(ROOT_DIRECTORY_PATH); for (const catalogDir of catalogDirs) { const catalogIndexFilePath = path.join(catalogDir, \"catalog_index.json\"); const catalogIndex = JSON.parse( fs.readFileSync(catalogIndexFilePath, \"utf8\") ); const collectionName = catalogIndex.catalog; if (!collectionName) { console.error( `Catalog name not found in the catalog index file at ${catalogIndexFilePath}. Skipping...` ); continue; } const collection = db.collection(collectionName); // Ensure unique index on levelName field await collection.createIndex({ levelName: 1 }, { unique: true }); console.log( `Uploading directory: ${catalogDir} to collection: ${collectionName}` ); await traverseAndUpload(catalogDir, collection); console.log(`Upload completed for directory: ${catalogDir}`); } console.log(\"All catalogs have been uploaded successfully.\"); } catch (error) { if (error.code === 11000) { console.error(\"Duplicate entry found. Skipping upload.\"); } else { console.error(\"Error uploading to MongoDB:\", error.message); } } finally { await client.close(); } } function findCatalogDirectories(directoryPath: string): string[] { const catalogDirs = [] as any; function recurseDirs(currentPath: string) { const items = fs.readdirSync(currentPath); for (const item of items) { const itemPath = path.join(currentPath, item); const stats = fs.statSync(itemPath); if (stats.isDirectory()) { const catalogIndexPath = path.join(itemPath, \"catalog_index.json\"); if (fs.existsSync(catalogIndexPath)) { catalogDirs.push(itemPath); } else { recurseDirs(itemPath); // Recurse into subdirectories } } } } recurseDirs(directoryPath); return catalogDirs; } async function traverseAndUpload(directoryPath: string, collection: any) { const items = fs.readdirSync(directoryPath); for (const item of items) { const itemPath = path.join(directoryPath, item); const stats = fs.statSync(itemPath); if (stats.isDirectory()) { console.log(`Uploading level directory: ${itemPath}`); const levelDocuments = await processLevelDirectory(itemPath); // Check for duplicate before inserting const existingDocument = await collection.findOne({ levelName: item }); if (!existingDocument) { await collection.insertOne({ levelName: item, files: levelDocuments, }); console.log(`Uploaded level: ${item}`); } else { console.log(`Skipping duplicate level: ${item}`); } } } } async function processLevelDirectory(directoryPath: string) { const items = fs.readdirSync(directoryPath); const levelDocuments = [] as any; for (const item of items) { const itemPath = path.join(directoryPath, item); const stats = fs.statSync(itemPath); if (stats.isFile()) { console.log(`Processing file: ${itemPath}`); const fileContent = fs.readFileSync(itemPath); levelDocuments.push({ type: \"file\", name: item, content: fileContent.toString(\"base64\"), // Store file content as Base64 }); } } return levelDocuments; } uploadAllCatalogsToMongoDB();"}},"downloadArchivedLevelData.ts":{"content":"import fs from \"fs\"; import path from \"path\"; import axios from \"axios\"; import * as dotenv from \"dotenv\"; dotenv.config({ path: \".env.local\" }); const CATALOG_DIR: string = process.env.MMT_ARCHIVED_CATALOG_DIR; const ensureDirectoryExists = async (dir: fs.PathLike) => { try { await fs.promises.stat(dir); } catch { await fs.promises.mkdir(dir, { recursive: true }); } }; const readJsonFile = async (filePath: string): Promise<any> => { try { const fileData = await fs.promises.readFile(filePath, \"utf8\"); return JSON.parse(fileData); } catch (error) { console.error(`Error reading JSON file at ${filePath}:`, error); throw error; } }; const downloadFile = async (fileUrl: string, savePath: string) => { try { await ensureDirectoryExists(path.dirname(savePath)); const response = await axios({ url: fileUrl, method: \"GET\", responseType: \"stream\", }); await new Promise((resolve, reject) => { const writer = fs.createWriteStream(savePath); response.data.pipe(writer); writer.on(\"finish\", resolve); writer.on(\"error\", reject); }); } catch (error) { console.error(`Error downloading file from ${fileUrl}:`, error); } }; const processCatalogFile = async (catalogPath: string) => { try { const catalog = await readJsonFile(catalogPath); const parentDir = path.dirname(catalogPath); for (const file of catalog.files) { let savePath = path.join(parentDir, file.fileName); if ( file.fileUrl.includes(\"/old/\") || file.fileUrl.includes(\"/old version/\") ) { const ext = path.extname(file.fileName); const baseName = path.basename(file.fileName, ext); savePath = path.join(parentDir, `${baseName}_old${ext}`); } else if (file.fileUrl.includes(\"/fixed version/\")) { savePath = path.join(parentDir, file.fileName); } console.log(`Downloading ${file.fileUrl} to ${savePath}`); await downloadFile(file.fileUrl, savePath); } if (catalog.thumbnailUrl) { const thumbnailPath = path.join( parentDir, path.basename(catalog.thumbnailUrl) ); console.log( `Downloading thumbnail ${catalog.thumbnailUrl} to ${thumbnailPath}` ); await downloadFile(catalog.thumbnailUrl, thumbnailPath); } } catch (error) { console.error(`Error processing catalog file at ${catalogPath}:`, error); } }; const traverseAndDownload = async (dir: string) => { const entries = await fs.promises.readdir(dir, { withFileTypes: true }); for (const entry of entries) { const fullPath = path.join(dir, entry.name); if (entry.isDirectory()) { if ( entry.name.toLowerCase() === \"old\" || entry.name.toLowerCase() === \"old version\" || entry.name.toLowerCase() === \"fixed version\" ) { continue; } const catalogPath = path.join(fullPath, \"catalog.json\"); if (fs.existsSync(catalogPath)) { await processCatalogFile(catalogPath); } await traverseAndDownload(fullPath); } } }; const downloadArchivedLevelData = async () => { await ensureDirectoryExists(CATALOG_DIR); await traverseAndDownload(CATALOG_DIR); }; const init = async () => { await downloadArchivedLevelData(); }; init(); export { downloadArchivedLevelData };"},"fetchHognoseMaps.ts":{"content":"/** * This script fetches the latest procedurally generated levels from the Hognose project * and extracts only the .dat files to a specified directory. It downloads the latest * release zip file from the Hognose GitHub repository, ensures necessary directories * are created, and logs relevant information. * * All LevelPak files are credited to the Hognose project by charredUtensil. * GitHub Repository: https://github.com/charredUtensil/hognose * * The creator, charredUtensil, has made a set of 256 procedurally generated levels * for the game Manic Miners available for free. We decided to include them in our * level indexing system to enhance our collection and provide more content. */ import axios from \"axios\"; import fs from \"fs\"; import path from \"path\"; import unzipper from \"unzipper\"; import * as dotenv from \"dotenv\"; dotenv.config({ path: \".env.local\" }); const GITHUB_API_URL = \"https://api.github.com/repos/charredUtensil/hognose/releases/latest\"; const DOWNLOAD_DIR = process.env.HOGNOSE_LEVELS_DOWNLOAD_DIR; if (!DOWNLOAD_DIR) { console.error( \"Error: HOGNOSE_LEVELS_DOWNLOAD_DIR must be set in the .env file.\" ); process.exit(1); } const ZIP_FILE_NAME = \"levelpak.zip\"; const INDEX_FILE_NAME = \"hognose_index.json\"; const INDEX_FILE_PATH = path.resolve(DOWNLOAD_DIR, INDEX_FILE_NAME); async function fetchHognoseMaps() { try { console.log(\"Starting fetchHognoseMaps function...\"); console.log(`Fetching the latest release from: ${GITHUB_API_URL}`); const response = await axios.get(GITHUB_API_URL); const releaseInfo = response.data; const assets = releaseInfo.assets; const levelPak = assets.find( (asset: { name: string }) => asset.name === ZIP_FILE_NAME ); if (!levelPak) { throw new Error(`${ZIP_FILE_NAME} not found in the latest release.`); } const downloadUrl = levelPak.browser_download_url; const zipFilePath = path.resolve(DOWNLOAD_DIR, ZIP_FILE_NAME); const writer = fs.createWriteStream(zipFilePath); console.log(`Downloading the file from: ${downloadUrl}`); const { data } = await axios({ url: downloadUrl, method: \"GET\", responseType: \"stream\", }); data.pipe(writer); await new Promise((resolve, reject) => { writer.on(\"finish\", resolve); writer.on(\"error\", reject); }); console.log(`${ZIP_FILE_NAME} downloaded successfully to ${DOWNLOAD_DIR}`); console.log( `Credits to charredUtensil for creating the Hognose project. Repository: https://github.com/charredUtensil/hognose` ); let datFiles = [] as any; await fs .createReadStream(zipFilePath) .pipe(unzipper.Parse()) .on(\"entry\", async (entry) => { const fileName = entry.path; const type = entry.type; const isDatFile = fileName.endsWith(\".dat\"); if (type === \"File\" && isDatFile) { const outputPath = path.resolve( DOWNLOAD_DIR, path.basename(fileName) ); entry.pipe(fs.createWriteStream(outputPath)); datFiles.push(path.basename(fileName)); } else { entry.autodrain(); } }) .promise(); console.log(`Extracted .dat files: ${datFiles}`); const indexData = { version: releaseInfo.tag_name, downloadedAt: new Date().toISOString(), repository: \"https://github.com/charredUtensil/hognose\", datFiles: datFiles, }; fs.writeFileSync(INDEX_FILE_PATH, JSON.stringify(indexData, null, 2)); fs.unlinkSync(zipFilePath); console.log(`.dat files extracted: ${datFiles.length}`); console.log(`${ZIP_FILE_NAME} has been deleted after extraction.`); console.log( `hognose_index.json created with release version, download date, repository URL, and list of .dat files.` ); } catch (error) { console.error(\"Error fetching and extracting .dat files:\", error.message); throw error; } } async function checkForUpdate() { try { console.log(\"Starting checkForUpdate function...\"); if (fs.existsSync(INDEX_FILE_PATH)) { console.log(\"hognose_index.json exists, reading file...\"); const indexData = JSON.parse(fs.readFileSync(INDEX_FILE_PATH, \"utf8\")); const response = await axios.get(GITHUB_API_URL); const latestRelease = response.data; const latestVersion = latestRelease.tag_name; if (indexData.version !== latestVersion) { console.log( `New version detected: ${latestVersion}. Clearing directory and downloading new files.` ); fs.rmSync(DOWNLOAD_DIR, { recursive: true, force: true }); fs.mkdirSync(DOWNLOAD_DIR, { recursive: true }); await fetchHognoseMaps(); } else { const datFiles = fs .readdirSync(DOWNLOAD_DIR) .filter((file) => file.endsWith(\".dat\")); if (datFiles.length !== indexData.datFiles.length) { console.log(\"Mismatch in .dat file count. Redownloading files.\"); fs.rmSync(DOWNLOAD_DIR, { recursive: true, force: true }); fs.mkdirSync(DOWNLOAD_DIR, { recursive: true }); await fetchHognoseMaps(); } else { console.log(\"No updates needed. The .dat files are up to date.\"); } } } else { console.log( \"hognose_index.json does not exist, performing initial download...\" ); await fetchHognoseMaps(); } } catch (error) { console.error(\"Error checking for updates:\", error.message); throw error; } } (async () => { try { console.log(\"Starting script...\"); if (!fs.existsSync(DOWNLOAD_DIR)) { console.log(`Creating download directory: ${DOWNLOAD_DIR}`); fs.mkdirSync(DOWNLOAD_DIR, { recursive: true }); } await checkForUpdate(); console.log(\"Script completed successfully.\"); } catch (error) { console.error( \"Failed to check for updates and fetch .dat files:\", error.message ); } })();"},"generatePNG.ts":{"content":"import * as dotenv from \"dotenv\"; dotenv.config({ path: \".env.local\" }); const fs = require(\"fs\").promises; import path from \"path\"; import { generatePNG } from \"../src/functions/generatePNG\"; import { create2DArray } from \"../src/functions/create2DArray\"; import { parseMapDataFromFile } from \"../fileParser/mapFileParser\"; export async function generatePNGFromFiles( filePaths: string[] | string ): Promise<{ success: boolean }[]> { const files = Array.isArray(filePaths) ? filePaths : [filePaths]; const results = []; for (const filePath of files) { const outputDir = path.dirname(filePath); const outputFilePath = path.join(outputDir, \"screenshot_render.png\"); try { await fs.access(outputFilePath); console.log(\"File already exists:\", outputFilePath); results.push({ success: true, filePath: outputFilePath }); continue; } catch {} try { const parsedData = await parseMapDataFromFile({ filePath }); const wallArray = create2DArray({ data: parsedData.tilesArray, width: parsedData.colcount, }); const image = await generatePNG(wallArray, parsedData.biome); await image.toFile(outputFilePath); console.log(\"Image padded with border and saved as \" + outputFilePath); results.push({ success: true, filePath: outputFilePath }); } catch (error) { console.error(\"Error processing file:\", filePath, error); results.push({ success: false, filePath: outputFilePath }); } } return results; } async function findAllDatFiles(dir: any) { let results: any[] = []; const entries = await fs.readdir(dir, { withFileTypes: true }); for (const entry of entries) { const fullPath = path.join(dir, entry.name); if (entry.isDirectory()) { results = results.concat(await findAllDatFiles(fullPath)); } else if (entry.isFile() && entry.name.endsWith(\".dat\")) { results.push(fullPath); } } return results; } export async function processDirectory(datDirectory: string) { const datFiles = await findAllDatFiles(datDirectory); const results = []; for (const filePath of datFiles) { const outputDir = path.dirname(filePath); const outputFilePath = path.join(outputDir, \"screenshot_render.png\"); try { await fs.access(outputFilePath); console.log(\"File already exists:\", outputFilePath); results.push({ success: true, filePath: outputFilePath }); continue; } catch {} try { const parsedData = await parseMapDataFromFile({ filePath }); const wallArray = create2DArray({ data: parsedData.tilesArray, width: parsedData.colcount, }); const image = await generatePNG(wallArray, parsedData.biome); await image.toFile(outputFilePath); console.log(\"Image padded with border and saved as \" + outputFilePath); results.push({ success: true, filePath: outputFilePath }); } catch (error) { console.error(\"Error processing file:\", filePath, error); results.push({ success: false, filePath: outputFilePath }); } } return results; } async function init() { const directoryPath = process.env.MMT_GENERATE_PNG_DIR; const processingResults = await processDirectory(directoryPath); console.log(processingResults); } init();"},"logMapDataStats.ts":{"content":"import * as os from \"os\"; import * as fs from \"fs/promises\"; import * as path from \"path\"; import * as dotenv from \"dotenv\"; import * as readline from \"readline\"; import { ParsedMapData } from \"../fileParser/types\"; import { parseMapDataFromFile } from \"../fileParser/mapFileParser\"; dotenv.config({ path: \".env.local\" }); const rl = readline.createInterface({ input: process.stdin, output: process.stdout, }); async function logMapDataStats(baseDir: string): Promise<any> { let fileCount = 0; let failedCount = 0; const failedFiles: string[] = []; const mapDataResults: ParsedMapData[] = []; async function traverseDirectory(directory: string): Promise<void> { try { const directoryContents = await fs.readdir(directory, { withFileTypes: true, }); for (const dirent of directoryContents) { const fullPath = path.join(directory, dirent.name); if (dirent.isDirectory()) { await traverseDirectory(fullPath); } else if (dirent.name.endsWith(\".dat\")) { fileCount++; try { const mapData = await parseMapDataFromFile({ filePath: fullPath }); mapDataResults.push(mapData); } catch (parseError) { console.error( `[ERROR] Failed to parse file ${fullPath}: ${parseError}` ); failedCount++; failedFiles.push(fullPath); } } } } catch (dirError) { console.error( `[ERROR] Error accessing directory ${directory}: ${dirError}` ); } } await traverseDirectory(baseDir); const result = { processedFiles: fileCount, failedFiles: failedCount, mapDataResults, failedFilesDetails: failedFiles, }; console.log(\"========== Map Data Parsing Results ==========\"); console.log(`Processed files: ${fileCount}`); console.log(`Failed to process files: ${failedCount}`); if (failedCount > 0) { console.log(\"Failed file paths:\"); failedFiles.forEach((file) => console.log(file)); } return result; } async function init() { try { const directoryPath = process.env.MMT_CATALOG_DIR; rl.question( `The directory to be processed is: ${directoryPath}. Would you like to proceed? (yes/no): \\n`, async (answer) => { if (answer.toLowerCase() === \"yes\") { const processingResults = await logMapDataStats(directoryPath); console.log(\"Parsed Map Data:\"); console.log(processingResults); console.log(\"=================================================\"); } else { console.log(\"[INFO] Process aborted by user.\"); } rl.close(); } ); } catch (err) { console.error(\"[ERROR] Error initializing map data parsing:\", err); } } init();"},"mapIntegrityCheck.ts":{"content":"import * as dotenv from \"dotenv\"; import * as fs from \"fs/promises\"; import * as path from \"path\"; import * as chardet from \"chardet\"; import { Stats } from \"fs\"; import { colors } from \"../src/functions/colorMap\"; dotenv.config({ path: \".env.local\" }); async function mapTileIntegrityCheck(baseDir: string): Promise<any> { const allTiles = new Set<number>(); const tileOccurrences: { [key: number]: number } = {}; const mapTileSets: Set<number>[] = []; let fileCount = 0; let failedCount = 0; let directoriesChecked = 0; let directoriesWithDatFiles = 0; const failedFiles: { path: string; size: number; metadata: Stats; encoding: string; }[] = []; async function traverseDirectory(directory: string): Promise<void> { directoriesChecked++; let datFileFound = false; try { const directoryContents = await fs.readdir(directory, { withFileTypes: true, }); for (const dirent of directoryContents) { const fullPath = path.join(directory, dirent.name); if (dirent.isDirectory()) { await traverseDirectory(fullPath); } else if (dirent.name.endsWith(\".dat\")) { datFileFound = true; try { const encoding = chardet.detectFileSync(fullPath) || \"utf8\"; const data = await fs.readFile(fullPath, \"utf8\"); const normalizedData = data .replace(/\\r\\n/g, \"\\n\") .replace(/\\r/g, \"\\n\"); const tilesArray = extractTilesArray(normalizedData); if (tilesArray) { const tileSet = new Set(tilesArray); mapTileSets.push(tileSet); tilesArray.forEach((tile) => { allTiles.add(tile); tileOccurrences[tile] = (tileOccurrences[tile] || 0) + 1; }); fileCount++; } else { console.log( `[FAIL] Failed to extract tiles from file: ${fullPath}` ); const stats = await fs.stat(fullPath); failedFiles.push({ path: fullPath, size: stats.size, metadata: stats, encoding, }); failedCount++; } } catch (readError) { console.error( `[ERROR] Error reading file ${fullPath}: ${readError}` ); const stats = await fs.stat(fullPath); const encoding = chardet.detectFileSync(fullPath) || \"utf8\"; failedFiles.push({ path: fullPath, size: stats.size, metadata: stats, encoding, }); failedCount++; } } } } catch (dirError) { console.error( `[ERROR] Error accessing directory ${directory}: ${dirError}` ); } if (datFileFound) { directoriesWithDatFiles++; } } function extractTilesArray(fileContent: string): number[] | null { try { const tilesMatch = fileContent.match(/tiles\\s*\\{\\s*([^}]*)\\s*\\}/); if (tilesMatch) { const tilesString = tilesMatch[1]; const tilesArray = tilesString .split(/[\\s,]+/) .map((num) => num.trim()) .filter((num) => num.length > 0 && num !== \"-\") // Filter out empty and invalid entries .map((num) => parseInt(num, 10)) .filter((num) => !isNaN(num)); return tilesArray; } return null; } catch (error) { console.error( `[ERROR] Error extracting tiles array from content:`, error ); return null; } } await traverseDirectory(baseDir); let commonTiles = new Set<number>([...allTiles]); mapTileSets.forEach((tileSet) => { commonTiles = new Set([...commonTiles].filter((tile) => tileSet.has(tile))); }); const standoutTiles = [...allTiles].filter( (tile) => !colors.hasOwnProperty(tile) ); const uniqueTiles = Object.keys(tileOccurrences) .map(Number) .filter((tile) => tileOccurrences[tile] === 1); const result = { processedFiles: fileCount, failedFiles: failedCount, directoriesChecked, directoriesWithDatFiles, totalUniqueTiles: allTiles.size, standoutTiles, uniqueTiles, failedFilesDetails: failedFiles, }; console.log(\"========== Map Integrity Check Summary ==========\"); console.log(`Processed files: ${fileCount}`); console.log(`Failed to process files: ${failedCount}`); console.log(`Directories checked: ${directoriesChecked}`); console.log(`Directories with .dat files: ${directoriesWithDatFiles}`); console.log( `Standout tiles (not matching colormap): ${standoutTiles.join(\", \")}` ); if (failedFiles.length > 0) { console.log(\"========== Failed Files ==========\"); failedFiles.forEach((file) => { console.log(\"----------------------------------\"); console.log(`File: ${file.path}`); console.log(`Size: ${file.size} bytes`); console.log(`Encoding: ${file.encoding}`); console.log(`Metadata:`, file.metadata); }); console.log(\"==================================\"); } return result; } async function init() { const directoryPath = process.env.MMT_CATALOG_DIR; const processingResults = await mapTileIntegrityCheck(directoryPath); console.log(processingResults); } init().catch((err) => console.error(\"[ERROR] Error initializing map tile integrity check:\", err) );"},"minifyProject.ts":{"content":"import * as fs from \"fs/promises\"; import * as path from \"path\"; import * as dotenv from \"dotenv\"; dotenv.config({ path: \".env.local\" }); async function minifyDirectory( baseDir: string, ignoreList: string[] = [] ): Promise<any> { let directoryStructure: any = {}; async function traverseDirectory( currentPath: string, structure: any ): Promise<void> { const directoryContents = await fs.readdir(currentPath, { withFileTypes: true, }); for (const dirent of directoryContents) { if (ignoreList.includes(dirent.name)) continue; const fullPath = path.join(currentPath, dirent.name); if (dirent.isDirectory()) { console.log(`Directory: ${fullPath}`); structure[dirent.name] = {}; await traverseDirectory(fullPath, structure[dirent.name]); } else { console.log(`File: ${fullPath}`); const fileContent = await fs.readFile(fullPath, \"utf8\"); const minifiedContent = fileContent.replace(/\\s+/g, \" \").trim(); structure[dirent.name] = { content: minifiedContent, }; } } } await traverseDirectory(baseDir, directoryStructure); return directoryStructure; } async function init() { try { const baseDir = process.cwd(); const outputDir = process.env.MMT_OUTPUT_DIR || baseDir; const ignoreList = [ \"dist\", \"assets\", \"node_modules\", \".git\", \".vscode\", \"README.md\", \"package-lock.json\", ]; const outputFilePath = path.join(outputDir, \"directory_structure.json\"); // Wipe the output file if it exists try { await fs.unlink(outputFilePath); console.log(`[INFO] Existing output file '${outputFilePath}' deleted.`); } catch (err) { if (err.code !== \"ENOENT\") { console.error(\"[ERROR] Error deleting existing output file:\", err); } } const result = await minifyDirectory(baseDir, ignoreList); console.log(\"========== Directory Structure ==========\"); console.log(JSON.stringify(result, null, 2)); console.log(\"=========================================\"); await fs.writeFile(outputFilePath, JSON.stringify(result)); console.log( \"[INFO] Directory structure has been written to directory_structure.json\" ); } catch (err) { console.error(\"[ERROR] Error minifying directory:\", err); } } init();"}},"src":{"functions":{"colorMap.ts":{"content":"import { Color } from \"../types/types\"; export const colors: { [key: number | string]: Color | any } = { 1: { r: 124, g: 92, b: 70 }, // Ground 5: { r: 92, g: 58, b: 40 }, // not sure maybe hot rock almost lava 6: { r: 255, g: 50, b: 0 }, // Lava 7: { r: 0, g: 0, b: 255 }, // Placeholder color 8: { r: 255, g: 255, b: 0 }, // Placeholder color 9: { r: 75, g: 0, b: 130 }, // Placeholder color 10: { r: 0, g: 255, b: 0 }, // Placeholder color 11: { r: 30, g: 84, b: 197 }, // Water (Teal Blue) 12: { r: 180, g: 180, b: 20 }, // Slimy Slug hole 14: { r: 220, g: 220, b: 220 }, // Building power path 24: { r: 180, g: 150, b: 100 }, // Placeholder color 26: { r: 169, g: 109, b: 82 }, // Dirt 30: { r: 139, g: 104, b: 86 }, // Loose Rock 34: { r: 77, g: 53, b: 50 }, // Hard Rock 38: { r: 0, g: 0, b: 0, a: 0 }, // Solid Rock, fully transparent 42: { r: 206, g: 233, b: 104 }, // Energy Crystal Seam 46: { r: 200, g: 85, b: 30 }, // Ore Seam 50: { r: 255, g: 255, b: 70 }, // Recharge Seam 60: { r: 46, g: 23, b: 95, alpha: 0.1 }, // Landslide rubble 61: { r: 46, g: 23, b: 95, alpha: 0.5 }, // Landslide rubble 63: { r: 46, g: 23, b: 95 }, // Landslide rubble 65: { r: 128, g: 0, b: 128 }, // Placeholder color 101: { r: 124, g: 92, b: 70 }, // Ground 102: { r: 173, g: 216, b: 230 }, // Placeholder color 103: { r: 100, g: 100, b: 100 }, // Placeholder color 104: { r: 255, g: 192, b: 203 }, // Placeholder color 105: { r: 90, g: 60, b: 30 }, // Placeholder color 106: { r: 255, g: 70, b: 10, alpha: 0.9 }, // Lava 107: { r: 0, g: 255, b: 127 }, // Placeholder color 108: { r: 255, g: 0, b: 255 }, // Placeholder color 109: { r: 255, g: 165, b: 0 }, // Placeholder color 110: { r: 0, g: 128, b: 128 }, // Placeholder color 111: { r: 30, g: 95, b: 220 }, // Water 112: { r: 180, g: 180, b: 20 }, // Slimy Slug hole 114: { r: 220, g: 220, b: 220 }, // Building power path 115: { r: 220, g: 220, b: 220 }, // dunno 124: { r: 70, g: 130, b: 180, alpha: 0.9 }, // Floating Flat Panels? 160: { r: 255, g: 0, b: 0 }, // Placeholder color 161: { r: 238, g: 130, b: 238 }, // Placeholder color 162: { r: 34, g: 139, b: 34 }, // Placeholder color 163: { r: 46, g: 23, b: 95 }, // Landslide rubble 164: { r: 65, g: 33, b: 95 }, // Landslide rubble 165: { r: 46, g: 23, b: 95, alpha: 0.5 }, // Weird Rubble? rock: { r: 120, g: 115, b: 110, alpha: 0.2 }, // BIOME BORDER COLOR Rocky, natural grey with transparency lava: { r: 255, g: 50, b: 0, alpha: 0.2 }, // BIOME BORDER COLOR Lava, orange with transparency ice: { r: 150, g: 200, b: 240, alpha: 0.2 }, // More bluish for ice biome };"},"create2DArray.ts":{"content":"/** * Creates a 2D array from a flat array of numbers by grouping elements into subarrays. * Each subarray will contain a specified number of elements, corresponding to the 'width' parameter. * * @param {number[]} data - The flat array of numbers to be converted into a 2D array. * @param {number} width - The number of elements each subarray should contain, representing the width of the 2D array. * @returns {number[][]} A 2D array where each subarray represents a row in the grid. */ export function create2DArray({ data, width, }: { data: number[]; width: number; }): number[][] { let result: number[][] = []; // Initialize the resulting 2D array // Iterate through the flat array in steps of 'width' to create subarrays for (let i = 0; i < data.length; i += width) { // Slice the flat array from the current index up to 'width' elements and add to the result result.push(data.slice(i, i + width)); } return result; // Return the newly created 2D array }"},"drawMapTiles.ts":{"content":"import { colors } from \"./colorMap\"; import { drawTile } from \"./drawTile\"; import { CanvasRenderingContext2D } from \"canvas\"; export async function drawMapTiles( ctx: CanvasRenderingContext2D, wallArray: string | any[], scale: number ) { for (let y = 0; y < wallArray.length; y++) { for (let x = 0; x < wallArray[0].length; x++) { const tile = wallArray[y][x]; const color = colors[tile] || { r: 255, g: 255, b: 255, a: 1 }; drawTile(ctx, x, y, scale, color); } } }"},"drawTile.ts":{"content":"import { Color } from \"../types/types\"; import { CanvasRenderingContext2D, createCanvas } from \"canvas\"; export function drawTile( ctx: CanvasRenderingContext2D, i: number, j: number, scale: number, color: Color ) { // Create a pattern canvas for detailed texture drawing const patternCanvas = createCanvas(scale, scale); const patternCtx = patternCanvas.getContext(\"2d\"); // Define a gradient for visual depth const tileGradient = patternCtx.createLinearGradient(0, 0, scale, scale); tileGradient.addColorStop( 0, `rgba(${color.r}, ${color.g}, ${color.b}, ${color.alpha || 1})` ); tileGradient.addColorStop( 1, `rgba(${Math.max(0, color.r - 20)}, ${Math.max( 0, color.g - 20 )}, ${Math.max(0, color.b - 20)}, ${color.alpha || 1})` ); patternCtx.fillStyle = tileGradient; patternCtx.fillRect(0, 0, scale, scale); // Add a subtle texture to the tile patternCtx.strokeStyle = \"rgba(255, 255, 255, 0.3)\"; patternCtx.lineWidth = 1; for (let k = 0; k < scale; k += 5) { patternCtx.beginPath(); patternCtx.moveTo(k, 0); patternCtx.lineTo(0, k); patternCtx.moveTo(scale, k); patternCtx.lineTo(k, scale); patternCtx.moveTo(k, scale); patternCtx.lineTo(scale, k); patternCtx.stroke(); } // Shadow and highlight for depth // Apply shadows for a raised tile effect ctx.shadowColor = \"rgba(0, 0, 0, 0.5)\"; ctx.shadowBlur = 5; ctx.shadowOffsetX = 3; ctx.shadowOffsetY = 3; // Apply the pattern const pattern = ctx.createPattern(patternCanvas, \"repeat\"); ctx.fillStyle = pattern; ctx.fillRect(j * scale, i * scale, scale, scale); // Draw highlight on the top and left edges for a beveled look ctx.strokeStyle = \"rgba(255, 255, 255, 0.8)\"; ctx.beginPath(); ctx.moveTo(j * scale, i * scale + scale); ctx.lineTo(j * scale, i * scale); ctx.lineTo(j * scale + scale, i * scale); ctx.stroke(); // Reset shadow to prevent it from affecting other elements ctx.shadowColor = \"transparent\"; ctx.shadowBlur = 0; ctx.shadowOffsetX = 0; ctx.shadowOffsetY = 0; }"},"generatePNG.ts":{"content":"import sharp from \"sharp\"; import { colors } from \"./colorMap\"; import { createCanvas } from \"canvas\"; import { drawMapTiles } from \"./drawMapTiles\"; export async function processImage( buffer: sharp.SharpOptions | Buffer | any, frameWidth: number, frameHeight: number, padding: number, biome: string ) { let image = sharp(buffer).sharpen(); // Resize image to fit within the frame dimensions, considering padding image = await image.resize( frameWidth - 2 * padding, frameHeight - 2 * padding, { fit: \"inside\", withoutEnlargement: true, } ); // Extend the resized image with padding and colored background image = await image.extend({ top: padding, bottom: padding, left: padding, right: padding, background: colors[biome.toLowerCase()] || { r: 0, g: 0, b: 0, alpha: 0.31, }, }); // Rotate the image if it is in landscape (width > height) const metadata = await image.metadata(); if (metadata.width > metadata.height) { image = image.rotate(90); } return image; } export async function generatePNG( wallArray: string | any[], biome = \"default\" ) { const scale = 17; const width = wallArray.length; const height = wallArray[0].length; const canvas = createCanvas(width * scale, height * scale); const ctx = canvas.getContext(\"2d\"); await drawMapTiles(ctx, wallArray, scale); const buffer = canvas.toBuffer(\"image/png\"); const frameWidth = 1280; const frameHeight = 1080; const padding = 25; const image = await processImage( buffer, frameWidth, frameHeight, padding, biome ); const finalCanvas = await image.toBuffer(); return await sharp(finalCanvas).resize(1280, 1280, { fit: \"contain\", background: { r: 0, g: 0, b: 0, alpha: 0.1 }, }); }"},"generateThumbnail.ts":{"content":"import { createCanvas, CanvasRenderingContext2D } from \"canvas\"; import sharp from \"sharp\"; import { colors } from \"./colorMap\"; // Adjust the path as needed export async function generateThumbnail(wallArray: string | any[]) { const scale = 5; // Smaller scale for thumbnails const width = wallArray.length; const height = wallArray[0].length; const canvas = createCanvas(width * scale, height * scale); const ctx = canvas.getContext(\"2d\"); await drawMapThumbTiles(ctx, wallArray, scale); const buffer = canvas.toBuffer(\"image/png\"); return await sharp(buffer) .resize(320, 320, { fit: \"contain\", background: { r: 0, g: 0, b: 0, alpha: 0.1 }, }) .toBuffer(); } async function drawMapThumbTiles( ctx: CanvasRenderingContext2D, wallArray: string | any[], scale: number ) { for (let y = 0; y < wallArray.length; y++) { for (let x = 0; x < wallArray[0].length; x++) { const tile = wallArray[y][x]; const color = colors[tile] || { r: 255, g: 255, b: 255, a: 1 }; drawThumbTile(ctx, x, y, scale, color); } } } function drawThumbTile( ctx: CanvasRenderingContext2D, y: number, x: number, scale: number, color: { r: number; g: number; b: number; a: number } ) { ctx.fillStyle = `rgba(${color.r}, ${color.g}, ${color.b}, ${color.a})`; ctx.fillRect(x * scale, y * scale, scale, scale); }"}},"types":{"types.ts":{"content":"export interface Color { r: number; g: number; b: number; alpha?: number; }"}}},"tsconfig.json":{"content":"{ \"compilerOptions\": { \"module\": \"commonjs\", \"esModuleInterop\": true, \"target\": \"es6\", \"noImplicitAny\": true, \"moduleResolution\": \"node\", \"sourceMap\": true, \"outDir\": \"dist\", \"baseUrl\": \".\", \"paths\": { \"*\": [\"node_modules/*\"] } }, \"include\": [ \"*.ts\", \"*.js\", \"src/**/*.ts\", \"scripts/*.ts\", \"scripts/*.js\", \"scripts/db/uploadHognoseCatalog.ts\", \"scripts/db/downloadCollectionFromMongoDB.ts\" ] }"},"utils":{"camelCaseString.ts":{"content":"export function camelCaseString(str: string) { const words = str .replace(/'/g, \"\") .replace(/[^a-zA-Z0-9\\s\\-]/g, \"\") .split(/[\\s\\-]+/); return words .map((word, index) => index === 0 ? word.toLowerCase() : word.charAt(0).toUpperCase() + word.slice(1).toLowerCase() ) .join(\"\"); }"},"constructDescriptions.ts":{"content":"import * as dotenv from \"dotenv\"; dotenv.config({ path: \".env.local\" }); export function constructDescription(parsedData: any): string { const descriptionParts = [] as any; descriptionParts.push( `This is a ${parsedData.sizeCategory} map with dimensions of ${parsedData.rowcount}x${parsedData.colcount} tiles.` ); if (parsedData.biome) { descriptionParts.push(`It features a ${parsedData.biome} biome.`); } if ( parsedData.minElevation !== null && parsedData.maxElevation !== null && parsedData.minElevation !== parsedData.maxElevation ) { descriptionParts.push( `The map has an elevation range from ${parsedData.minElevation} to ${parsedData.maxElevation}.` ); } if (parsedData.oreCount) { descriptionParts.push( `The map contains ${parsedData.oreCount} ore deposits.` ); } if (parsedData.crystalCount) { descriptionParts.push( `The map contains ${parsedData.crystalCount} crystals.` ); } if (parsedData.averageElevation !== null) { descriptionParts.push( `The average elevation of the map is ${parsedData.averageElevation}.` ); } descriptionParts.push(`Created using Hognose by charredUtensil.`); return descriptionParts.join(\" \"); } export function constructHtmlDescription(parsedData: any): string { const descriptionParts = [] as any; descriptionParts.push( `<div>This is a ${parsedData.sizeCategory} map with dimensions of ${parsedData.rowcount}x${parsedData.colcount} tiles.</div>` ); if (parsedData.biome) { descriptionParts.push( `<div>It features a ${parsedData.biome} biome.</div>` ); } if ( parsedData.minElevation !== null && parsedData.maxElevation !== null && parsedData.minElevation !== parsedData.maxElevation ) { descriptionParts.push( `<div>The map has an elevation range from ${parsedData.minElevation} to ${parsedData.maxElevation}.</div>` ); } if (parsedData.oreCount) { descriptionParts.push( `<div>The map contains ${parsedData.oreCount} ore deposits.</div>` ); } if (parsedData.crystalCount) { descriptionParts.push( `<div>The map contains ${parsedData.crystalCount} crystals.</div>` ); } if (parsedData.averageElevation !== null) { descriptionParts.push( `<div>The average elevation of the map is ${parsedData.averageElevation}.</div>` ); } descriptionParts.push(`<div>Created using Hognose by charredUtensil.</div>`); return descriptionParts.join(\"\\n\"); }"},"generateShortDescription.ts":{"content":"export function generateShortDescription(parsedData: any): string { const biomeAdjectives: Record<string, string[]> = { ice: [\"Icy\", \"Frozen\", \"Glacial\", \"Chilly\", \"Frosty\", \"Snowy\", \"Arctic\"], rock: [ \"Rocky\", \"Stony\", \"Craggy\", \"Solid\", \"Boulder-strewn\", \"Granitic\", \"Rugged\", ], lava: [ \"Molten\", \"Fiery\", \"Volcanic\", \"Blazing\", \"Searing\", \"Scorching\", \"Infernal\", ], }; const sizeDescriptors: Record<string, string[]> = { small: [\"Cavern\", \"Nook\", \"Crevice\", \"Grotto\", \"Hollow\", \"Burrow\"], medium: [\"Cave\", \"Chamber\", \"Vault\", \"Den\", \"Gallery\", \"Gulf\"], large: [ \"Expanse\", \"Abyss\", \"Underground Realm\", \"Subterranean World\", \"Cavernous Depths\", \"Enormous Cave\", ], }; const randomElement = (arr: string[]) => arr[Math.floor(Math.random() * arr.length)]; const biome = randomElement( biomeAdjectives[parsedData.biome.toLowerCase()] || [parsedData.biome] ); const size = randomElement( sizeDescriptors[parsedData.sizeCategory.toLowerCase()] || [ parsedData.sizeCategory, ] ); return `${biome} ${size}`; }"},"parseCatalogXmlToJson.ts":{"content":"import { parseStringPromise } from \"xml2js\"; import { camelCaseString } from \"../utils/camelCaseString\"; import { parse } from \"node-html-parser\"; import path from \"path\"; export async function parseCatalogXmlToJson( xml: string, dirPath: string, files: any[], thumbnailUrl: string ): Promise<any> { try { const result = await parseStringPromise(xml, { explicitArray: false, trim: true, mergeAttrs: true, }); const metadata = result.metadata; const originalTitle = metadata.title; const name = originalTitle.split(\"|\")[0].trim(); const title = camelCaseString(name); const collection = metadata.collection.split(\"-\"); const roomTitle = collection[2] ? collection[2].charAt(0).toUpperCase() + collection[2].slice(1) : \"Archive\"; const htmlDescription = metadata.description; const textDescription = parse(htmlDescription).innerText.trim(); const screenshotFile = files.find((file: any) => [\"PNG\", \"JPG\", \"JPEG\"].includes(file.fileType.toUpperCase()) ); const screenshot = screenshotFile ? screenshotFile.fileUrl : null; // Calculate the relative directory path const relativeDirPath = path.relative(process.cwd(), dirPath); const parsedJson = { catalog: roomTitle, catalogType: \"Level\", archived: true, pre_release: true, catalogId: metadata.identifier, title, postedDate: metadata.date, author: metadata.creator, thumbnail: thumbnailUrl, screenshot, shortDescription: \"\", textDescription, htmlDescription, path: relativeDirPath, url: `https://archive.org/details/${metadata.identifier}`, files, }; return parsedJson; } catch (error) { console.error(\"Error parsing XML to JSON:\", error); throw error; } }"},"parseXmlToJson.ts":{"content":"// Custom function to manually parse XML to JSON export function parseXmlToJson(xmlText: string) { const files: {}[] = []; const fileTags = xmlText.match(/<file[^>]*>(.*?)<\\/file>/gs); // Match each <file> block if (fileTags) { fileTags.forEach((fileTag: string) => { const file = {} as any; file.name = fileTag.match(/name=\"([^\"]+)\"/)?.[1]; file.source = fileTag.match(/source=\"([^\"]+)\"/)?.[1]; const properties = fileTag.match(/<(\\w+)>(.*?)<\\/\\1>/gs); // Match each property inside <file> if (properties) { properties.forEach((prop: string) => { const key = prop.match(/<(\\w+)>/)?.[1]; const value = prop.match(/>(.*?)</)?.[1]; if (key && value) { file[key] = value; } }); } files.push(file); }); } return files; }"}}}